# 2021

## May

### 2021-5-24

#### Implicit vs. explicit trust

To me, there are two flavors of trust. "Implicit" trust is trust derived from shared beliefs or opinions. "Explicit" trust, on the other hand, is trust that exists _despite_ a difference in beliefs or opinions.

When people say they trust others in personal or professional settings, they mean that they trust the judgment of the other party. Implicit trust is easy to explain: if both parties hold the same beliefs, it's reasonable to expect that both will arrive at similar conclusions when presented with a problem. In other words, the same input (situation) should lead to the same output (judgment) because both individuals share the same priors (beliefs). This is obvious; if I'm reasonably certain you would make the same decision as I would in a certain situation, why wouldn't I trust you?

Explicit trust is more interesting to me because it implies a level of emotionality and optimism due to incomplete information. The trustor does not know what conclusion the trustee will arrive at, yet still expresses confidence that the decision will be a favorable one. When framed this way, it becomes tempting to view explicit trust as inherently irrational. However, explicit trust seems to be a critical element of strong relationships (both personal and professional).

PS 1: What about trust that arises not from shared opinion, but from a pattern of positive outcomes? Someone's decision making process may be a black box to me, but if they consistently achieve good outcomes, I'd be inclined to trust them as well. This form of trust disregards priors entirely and only focuses on outcomes. I'm not sure if this is a reframing of explicit trust, or yet another flavor.

PS 2: The concept of explicit trust seems to have a lot in common with the concept of faith.

### 2021-5-21

#### Good reads lately

- [Horizon charts](https://observablehq.com/@d3/horizon-chart) - nice data visualization technique for line charts with multimodal peaks. Imagine a dataset where most values are between 1-100, but a subset spike to 1000-1100, and a further subset spike to ~10000. Compressing the Y-axis will destroy readability for the smaller ranges. Alternative to logarithmic scales for better glance value.
- [Uber - Designing Edge Gateway, Uber’s API Lifecycle Management Platform](https://eng.uber.com/gatewayuberapi/) - beyond being technically interesting, this article gives a sense of the time scales involved in building a massively scalable architecture. I think it also highlights that the viability of a design is a function of current business need.
- [Cindy Sridharan - Testing in Production, the safe way](https://copyconstruct.medium.com/testing-in-production-the-safe-way-18ca102d0ef1) - Cindy's articles are always a joy to read, jam-packed with information, and coupled with easily understandable examples.
- [Amazon builder's library](https://aws.amazon.com/builders-library/) - lots of gems here, but [Avoiding Insurmountable Queue Backlog](https://aws.amazon.com/builders-library/avoiding-insurmountable-queue-backlogs/?did=ba_card&trk=ba_card) and [Caching challenges and strategies](https://aws.amazon.com/builders-library/caching-challenges-and-strategies/?did=ba_card&trk=ba_card) stand out by being entertaining and well-written on top of being simply informative.

## February

### 2021-3-26

#### Legacy code: where is the tipping point?

There are some dumpster fires of codebases for which most viewers immediately agree "yeah, this is a lost cause; a full rewrite would be easier and cheaper." Less pathological cases cause more disagreement. What are the variables here?

The core one is logically "perceived difficulty of modifying the codebase." This might mean a lack of tests (no confidence in whether new changes introduce regressions), too many overly sensitive tests (even small changes enact a heavy refactoring burden), overly tight coupling (changes have many downstream effects), overly loose coupling (constituent parts are too hard to understand), limitations from core, unchangeable architectural decisions (e.g. performance problems), etc.

The key word to me is "perceived." Someone intimately familiar with the codebase may not be bothered by a lack of tests, as the tests are essentially in their brain (and are not particularly trustworthy, but what can you do). Similarly, they might know how to navigate the minefield of overly sensitive tests, instintively knowing what related tests must be updated. Similarly, someone with familiarity with DI, etc. might not view overly tight coupling as an intractable problem, as they know where to start to untangle the mess.

So really, the evaluation is a function of code quality, individual contextual knowledge, and individual expertise.

Individual contextual knowledge discouraging one from giving up on a dumpster fire project is suboptimal. As a first step, this contextual knowledge must be documented and made shareable. Even so, it's better to improve code quality than to use increased knowledge as a buffer for dealing with bad code. This is commonly seen by people getting "Stockholmed" after working on low quality codebases for a while -- they've picked up the coping mechanisms for productively working with the bad code.

Individual expertise discouraging one from giving up, on the other hand, is a good thing. While contextual knowledge represents how easy it is to cope with bad code, expertise represents how feasible it is to improve the bad code in the future.

A fourth variable is patience. Writing new code is funner than treading the minefield of existing code, and churning out new code makes people feel more productive. (Of course, there's no guarantee the rewrite will be any better than the old code, leading to an endless death-and-rebirth cycle.)

### 2021-2-25

#### [Macroeconomic populism](https://en.wikipedia.org/wiki/Macroeconomic_populism)

It seems like Phase 3 is where transitions to socialist economies break down. Capital flight strikes me as being the root cause.

Without explicit support from owners of capital, attempts at redistribution will be met with capital flight. To counter this, the government can:

1. Take this capital by force, either by preventing people from leaving with their wealth, or by simply seizing it. (China is the most immediate example.)
2. Preemptively kill all owners of capital (Southeast Asian Communists).
3. Attempt appeasement (which has apparently never worked) (1972 Chile).

When capital owners fight tooth and nail against redistribution, the end result is seemingly always either a brutal transition to totalitarianism (if the government is willing to dirty its hands), or collapse of the government (if not).

The only counterexample I can identify is Sweden. The buy-in from the wealthy seems to have arisen from lack of government corruption, a strong education system, a strong sense of camaraderie due to monoculturalism, and manageability due to the relatively small size of the country. The willingness of Swedes to cede many "freedoms" to the government is similar to Taiwan and its healthcare system, which requires a similar level of government involvement.

### 2021-2-16

#### Give and take

I've long considered my ability and drive to seize initiative as one of my greatest strengths. Despite not being a "natural manager," I am willing to grab the reins in a directionless project, or take responsibility for an undermanaged area.

This is one of the strengths I've consciously decided to give up while balancing childcare, personal and mental health, and work. I've decided that, at least for the short term, I will recede into the background. If I disagree with an opinion someone holds strongly, I will stay quiet.

My decision is an experiment as much as it is a coping mechanism. If work outcomes remain the same (or improve), then this is definitive evidence that I hold my opinions too strongly and crowd others out of decision making. The flip side of this strength is a glaring weakness: the mindset of always worrying about incorrect decisions being future landmines can cause conversations to be high-friction and emotionally charged.

### 2021-2-5

#### [A sober look at SPACs](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3720919)

Timely SPAC paper on HN front page today. TL;DR:

* One overlooked downside of SPACs is that how dilutive the merge ends up being is unclear up til the merge (because SPAC investors can choose whether or not to exchange their holdings). One consequence is that the claimed advantage of “price certainty” of SPACs vs. IPOs is overstated. Part 3 gets into this more (pp 18) and is the most interesting section IMO.
* Post-merger share prices tend to drop in price post-merger; people who exit their positions prior to the merger have better returns on average (resembles options in a way)
* On average, the larger the SPAC sponsor (e.g. if it’s a large private equity fund), the better the returns.
* SPACs being less regulated is a real advantage they have over IPOs as of now. NOTABLY, going public via SPAC doesn’t require you to file an S-1, so you’re a black box to retail investors and are essentially trading off of name brand alone (of both the company and the SPAC sponsor).
* SPAC sponsors take most of the returns, whereas SPAC shareholders end up bearing most of the risk/cost.
* TL;DR of the TL;DR: SPAC sponsors and SPAC IPO investors make out the best. SPAC holders who ride the wave and exit pre-merger make out ok. SPAC holders who hold post-merger are bagholders.

### 2021-2-1

#### Belated reflection post

Things I learned as an engineer this year (somewhat in the vein of [Chris Kiehl's popular post](https://chriskiehl.com/article/thoughts-after-6-years)):

- Perfect really is the enemy of good. I'm a tinkerer, which leads me to stumble upon local maxima of "elegant code." But elegant code isn't the end goal -- product (and value) are.
- Spiking and dictatorially making decisions on areas of high uncertainty isn't actually that bad... as long as you remain open to change and are not afraid to throw code away.
- Running meetings and scoping features democratically is suboptimal. Parallelize work aggressively, grouping up as needed rather than as a default.
- Decisions should be made before meetings, not during meetings.
- Unit tests are overrated on the frontend. Integration tests are underrated.
- Writing suboptimal (or straight up incorrect) code is okay if the subject is easy to change. Being able to identify what's easy to change is the mark of a good engineer.
- Set hard date requirements instead of hard feature requirements. This will act as a forcing function to make you scope correctly. Even if you don't scope correctly, at least you have stuff done.
- When working with unfamiliar tech, just start writing code. Read documentation like novels (front-to-back), not like dictionaries (random access).

## January

### 2021-1-21

#### Tips for big product launches

- Set **dates first** instead of **features first**. With featuers first, all dates end up being padded. Setting dates first forces you to really scope correctly and identify the MVP, and get to a state where you can start iterating. "Plan for 6 weeks and schedule for 3 weeks; plan to miss but aim to succeed."
- For the MVP, plan out acceptance tests, and adhere to them strictly. Use these as accountability tools for an objective measurement of progress. If an acceptance test is not passing by a deadline, really hone in on it.
- Learning how to correctly handle a big launch is hard. "This is why I don't recommend juniors join startups." At larger companies, you can fail without risking the existence of the company, and you can observe good practices. Because there are more large launches, you can evaluate them against each other for quality.

### 2021-1-8

#### [China containment policy](https://en.wikipedia.org/wiki/China_containment_policy)

I never thought history was interesting. In high school, I treated AP US History as simply another bundle of credits to jumpstart college.

The NYT published [How Neil Sheehan Got the Pentagon Papers](https://www.nytimes.com/2021/01/07/us/pentagon-papers-neil-sheehan.html) today, and it is an engrossing read. This got me reading more background about the papers, the Vietnam war, and the US' China containment policy.

* Much of the US' continued presence in the Middle East is due to this policy.
* I've always wondered why the US maintained such close ties to the Philippines. This explains it, at least in part.
* Most interestingly, much of the basis of the Vietnam War was in the name of this policy.
* Nixon's 1972 China visit marked a priority shift from containing China to containing Russia.
* Much of this comes down to the US' hatred of communism. American interventionism itself can largely be explained by anti-communism. A secondary rationale was the intent to not repeat the mistake of appeasement towards Nazi Germany pre-WWII.
* The [X Article](https://en.wikipedia.org/wiki/X_Article) gives some background into the US' ingrown anti-communism.
* One of Obama's big geopolitical moves in 2011 was a pivot of military resources out of the Middle East and into the APAC region.
* On the topic of Obama, the drone strike program was viewed as a black mark on his legacy. However, it's now known that the program was masterminded by the Bush administration, and during transition Bush pushed strongly for Obama to maintain this policy position. Similarly, Trump's trade war with China is seen as a black mark on his (fairly laughable) legacy; I wouldn't be surprised if this general policy position was carried over from the Obama administration.

### 2021-1-3

#### [HN: Uber discovered they’d been defrauded out of 2/3 of their ad spend](https://news.ycombinator.com/item?id=25623858)

Good discussion about the "ad-exchange DSP bubble," of which Rocket Fuel (my first company) was a part of. I'm sure glad to be a part of that any more.

* [Linked Twitter thread](https://twitter.com/nandoodles/status/1345774768746852353)
* [Podcast in question](https://www.alistdaily.com/lifestyle/kevin-frisch-uber-ad-fraud/)

### 2021-1-2

#### [Abstracting away correctness](https://fasterthanli.me/articles/abstracting-away-correctness)

(New year, new `.md`!)

This is a great post about language API quality. It's quite opinionated, but I thought the arguments were sound. The specific pattern under attack is the multiple-return pattern found in Go (and also commonly encountered in Node's error-first callbacks). I agree with the author's overall point that "misuse-resistant design" is a standard to strive for, and Rust definitely does a great job with this (at the cost of high up-front complexity, notably with the borrow checker). In contrast, the author claims that Go presents an exceedingly simple (and hence attractive) interface to developers; however, this is just a facade, with the true complexity arising as various gotchas.
