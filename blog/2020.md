# 2020

## May

### 2020-5-1

#### [Unison lang](https://www.unisonweb.org/docs/tour)

Still digesting this, but this is prime language nerd material: https://www.unisonweb.org/docs/tour

Video intro: https://www.youtube.com/watch?v=gCWtkvDQ2ZI (one of the most interesting tech talks I've watched recently!)

The language has an incredibly novel "build" system and entirely rethinks how source code is handled. Instead of treating a project as "a collection of text files," a Unison project is a giant, well-typed tree. Basically, all code is stored as a syntax tree and is keyed by the hash of the syntax tree (like Lisp taking to the logical limit). You modify code by "checking out" code from the abstract codebase (which generates a human-readable optimized version of the function from the AST), editing it, and "recommitting" it into the codebase. The codebase itself is basically a Merkle tree where every function's dependency is only tracked via the hash of the AST. The dev experience seems similar to what some companies with huge codebases like Google do, where you never have a complete, human-readable version of the codebase available and you create / update transient local source files that are somehow merged into the codebase.

The whole point of this convoluted system leads to some interesting properties:

It's a functional language with no build step. The "building" happens when you add code to the codebase via the CLI, and then it never needs to be type checked or verified again. If you modify any of its dependencies, it will be automatically updated when you commit the change to the dependency.
The codebase tracks all changes just like Git. Thus, you can essentially git log -p individual functions to see how they have changed over time. This in particular is super amazing to me.
No such thing as dependency version conflicts, because there's no longer a concept of a "namespace" -- in the codebase, everything is essentially in a global namespace and uniquely identified by hash. (Though there will still be semantic conflicts where an old dependency does something differently than a new one, etc.)
Code can be trivially executed across nodes in a distributed system. A single function can bounce execution across different execution contexts (using algebraic effects). When this happens, all required code is dynamically sent to the new context and executed. This is enabled by the fact that code "requirements" are always tracked, given the hashed AST system for referencing code.
"Renaming" functions is trivial, because internally, code references other code not by variable name, but by the hash of the code's AST. You can rename something once, and then every time you "check out" the code, all variable names will automatically be updated. Renaming a variable across an entire codebase will be a single-line diff.
You automatically get incredibly powerful metaprogramming capabilities (like Lisp, but better) since your codebase is a well-typed data structure you can arbitrarily transform.
Searching for things in the codebase is... interesting. You get a lot of really powerful things, like being able to query your codebase to "give me every function with a signature [Int] -> [String]" (give me every function that takes a list of integers and returns a list of strings). But you can't search for snippets of code like you currently can in any other language, and you can't grep or anything -- all searching has to be done through their CLI. And it seems like viewing code outside of their CLI like on Github is useless (since it'll just be a tree of hashes), though you can always dynamically regenerate an entire project in Unison code from the AST.

This is still all super experimental and impractical but the ideas explored here are some of the most interesting I've seen w.r.t. language design in a long time.

## April

### 2020-4-21

#### [AASM](https://github.com/aasm/aasm#guards)

Given how heavily we leverage AASM, I really should have spent the time to read the docs more carefully months ago.

When defining a state `active` and an event `activate`, AASM generates the following methods:

- `active?` - is state `active`.
- `activate!` - sends `activate` event; will error if current state cannot transition with this event.
- `may_activate?` - returns `true` if `activate!` would succeed. I wish I knew about this one ages ago. I didn't find it because I normally use `<TAB>` in pry to explore available methods.

#### [Who Y Combinator companies want](https://triplebyte.com/blog/who-y-combinator-companies-want)

Summary:

- Emphasize your practical, "boring" skills like accessibility, user testing, benchmarking, etc. over "ivory tower" ones like machine learning. Emphasize interest in product development and management over interest in hard academic topics (cryptography, ML...).
- Startups are unfairly biased against enterprise languages. It can help to seek out companies with the same background (e.g. Netflix is heavily invested into Java).
- Startups have a ton of variance, making it a numbers game. One prior bad hire who has a similar background as you can lead to you getting unfairly rejected.
- As a recruiter, try looking at academic / enterprise programmers and trying to tease out if they have or are interested in the "boring" skills you want.

### 2020-4-9

#### Lessons I've learned as a grad school dropout

I dropped out of grad school in December 2015 after two and a half grueling years. I left with practically nothing -- no first author publications, no degree, not even a single completed research project under my name. I hated my life so much that to me, losing a quarter of my twenties was a small price to pay for escape.

Roughly half a year later, I was starting my first day as a "real, professional software engineer." It's been almost four years since then, and I've seen some success in my new environment -- much, much more so than in academia, at least! The further I get from grad school, however, the more I'm able to objectively view my time there. In retrospect, it wasn't a waste at all: I've learned several invaluable things during my time in academia that continue to help me move forwards today.

#### Learning how to read

It didn't take me long to realize that for my entire life prior to grad school, _I had never learned to properly read._ While I don't consider myself to be particularly smart, I've always been a (comparatively) fast reader. I was skilled at identifying and extracting nuggets of important information from textbooks. This made me an efficient studier, so I did well on tests throughout school and college.

This model of learning quickly reached its limit in grad school. Research articles are incredibly information dense such that for the most part, _every sentence is important_, and simply extracting nuggets is not enough. This is also quickly impressed into you by the fact that unlike in college, all papers have a page _maximum_ instead of a page _minimum_. In addition, the sheer volume of reading that has to be done is several orders of magnitude higher than in college: if you are taking four courses, and each requires five articles per class, and each article is between 10 and 30 pages of minuscule text, you're basically consuming a textbook of highly compressed information a month. Reading is not a sprint, or a timed game of hide and seek. It is a slow, grueling, deliberate marathon. Once you are doing your own research, it's even worse: instead of having a handful of targeted articles prescribed to you, you have to trawl the entire ocean of prior research for the right articles.

When I became a junior engineer, I was stunned by the resistance many seemed to have against reading documentation. "It's too long. It takes too much time to find the information I need. I don't want to waste time going in circles." It was common to see people reaching for a more senior engineer the moment a roadblock was encountered (or worse, settling for a clearly insufficient solution). Even at that time, I strongly believed that the role of a junior wasn't to be as productive as possible or to produce the cleanest work possible, it was simply _to learn as much as possible, as quickly as possible_. I think I read the React documentation in its entirety at least once a month in my first six months. This was enough to set me apart from my peers and even some of my seniors in terms of subject matter expertise. So much information is readily available, with the only obstacle being volume.

To me, asking a senior and getting a single, targeted answer to a specific problem when a complete guidebook is readily available is akin to choosing a single nugget of insight over an untapped gold mine. The extra minutes or hours (or days, even) I'd spend wrangling with a problem _on my own terms_? A small price to pay.

#### Learning how to measure

I'll be frank. I remember very little from the thousands of pages I've read in grad school. I don't think I could give you five hypotheses, methods, and conclusions. I hardly even remember what classes I took. One thing I _do_ remember is the endless debates with my advisor and lab mates about what to measure to support our hypotheses. It's important to measure the right thing.

My field (industrial and organizational psychology) was somewhat unique in its application of abstract psychological principles to real organizations. Unlike in, say, neuroscience, I/O psychologists don't tend to measure psychological phenomena directly. We don't measure stress by swabbing cheeks, or attention by attaching electrodes to people's scalps, or motivation by doing a MRI. This is simply intractable at the organizational scale. Instead, we are constrained to picking good _proxy variables_, or measurable variables that can act as sufficient substitutes for unmeasurable ones. (You could say that biological phemonena are ultimately proxy variables as well, despite there being less layers of indirection.) When planning a study, it was always a laborious back-and-forth to argue about what real-world outcomes mapped to what psychological principles, how this mapping could be established, and how to actually measure this.

There is a parallel in engineering management. While we can directly measure engineering results by setting monitoring infrastructure, logging, and health checks, any attempt to do the same for productivity and motivation must use a proxy variable. Complicating the issue further is the fact that prior findings in psychological and management science often don't apply well to engineering organizations (especially young ones). The solutions for an organization are often dependent on the unique constraints and resources available to that organization. There is simply too much individual difference in startups. (Though, for the record, I think the higher level the finding, the more generally applicable it is. Conclusions about motivation, stress, and burnout are more universal than conclusions about leadership, performance measurement, and organizational change.)

My paltry 2.5 years studying organization psychology are nowhere near enough to have answers for the management challenges engineering organizations are facing. But they are more than enough to help me realize that these problems are anything but trivial, that anyone who claims to have a one-size-fits-all solution to this challenge is either misguided or has an agenda, that the effectiveness of managers is organization dependent, and that the manager that fits your organization is worth their weight in gold. It's hard as hell to know what to measure, and the answer can be different for every organization. But by knowing _how_ to measure, we can slowly get to the _what_ to measure that is unique to us.

#### Learning how to be content

The last is the most personal. Complaining around the water cooler or breakfast area is one of the most common tropes in office culture. It was especially prevalent at my first job (and sometimes went on until it was nearly lunchtime), and I've lost many hours of my life listening to people airing out their grievances about anything and everything, from the (lack of) leadership to the dwindling office amenities to the stagnant wages to the workload. They complained about how their old colleague was living it up at that big company in the Bay, and oh, how nice it must be.

And the whole, time, a single thought would resonate through my mind. FUCK OFF!!! No matter how bad my job got (and it got fairly toxic at the end), it never got anywhere _near_ how insanely difficult life in grad school was! I loathed living with four roomates. I loathed $1.39/lb chicken thigh from Walmart. I loathed seeing my friends with jobs enjoying their early twenties, trading their cheap hobbies like basketball for expensive ones like snowboarding. I loathed being seen as some kind of noble 21st century ascetic, foregoing a comfortable life in the pursuit of knowlege. I loathed it all, and threw my academic life in the garbage the instant I had the chance.

What did I trade it for? Certainly not for a job in a gleaming tower where they give out RSUs like candy. I don't have nap pods, and I don't have steak and sushi flown in to the office overnight cross country for lunch. People haven't ever heard of the companies I've worked for. But you know what? As a candidate with zero professional engineering experience, I was getting offers that were 500% my grad student stipend. Workload? As an engineer, you're considered a workaholic if you work on weekends; in grad school, take weekends off and you die. Perks? Okay, the health insurance is less than stellar compared to what students get, but otherwise they're uncomparable.

This isn't to discredit the individual struggles tech workers might face, or to say that the academic lifestyle is normal and not utterly unsustainable. However, to me it's incredibly important to maintain a sense of perspective. The five most valuable companies in the US are tech companies, and they're not shy about showing it. In the struggle to keep up with the Bezoses, it's easy to forget how easy tech workers have it overall. My wife is still in grad school and I live in a college town, so much of my friend group is still tied to academia in one way or another. I see flashes of the struggles I left behind, and it often fills me with guilt seeing how easy my life is in comparison. And it's not pity: ultimately, it's a fear of going back. What I have now, with all its flaws and day-to-day frustrations, is enough.

It's enough.

### 2020-4-5

#### [Slack's pre-launch memo](https://medium.com/@stewart/we-dont-sell-saddles-here-4c59524d650d)

Some interesting nuggets here:

1. Our position is different than the one many new companies find themselves in: we are not battling it out in a large, well-defined market with clear incumbents... and that means we can’t limit ourselves to tweaking the product; **we need to tweak the market too**.
2. What we are selling is not the software product — the set of all the features, in their specific implementation — because there are just not many buyers for this software product...
However, if we are selling “a reduction in the cost of communication” or “zero effort knowledge management” or “making better decisions, faster” or “all your team communication, instantly searchable, available wherever you go” or “75% less email” or some other valuable result of adopting Slack, we will find many more buyers. That’s why what we’re selling is organizational transformation. The software just happens to be the part we’re able to build & ship (and the means for us to get our cut).
3. The reason for saying we need to do ‘an exceptional, near-perfect job of execution’ is this: When you want something really bad, you will put up with a lot of flaws. But if you do not yet know you want something, your tolerance will be much lower. That’s why it is especially important for us to build a beautiful, elegant and considerate piece of software. Every bit of grace, refinement, and thoughtfulness on our part will pull people along. Every petty irritation will stop them and give the impression that it is not worth it. That means we have to find all those petty irritations, and quash them. We need to look at our own work from the perspective of a new potential customer and actually see what’s there. Does it make sense? Can you predict what’s going to happen when you click that button or open that menu? Is there sufficient feedback to know if the click or tap worked? Is it fast enough? If I read the email on my phone and click the link, is it broken?.. It is always harder to do this with one’s own product: we skip over the bad parts knowing that we plan to fix it later. We already know the model we’re using and the terms we use to describe it. It is very difficult to approach Slack with beginner’s mind. But we have to, all of us, and we have to do it every day, over and over and polish every rough edge off until this product is as smooth as lacquered mahogany.

## March

### 2020-3-25

#### Engineering empowerment

My current company is quite unique compared to previous places I've worked, in that every engineer essentially has carte blanche with regards to how they prioritize their work. This is actually quite a lot more stressful than I imagined it would be, because the onus is on you to make sure you are prioritizing the _right_ thing. In the past, I've always had a project manager steering the ship.

When your peers and your managers expect that you are talented enough to make the right call, you will second guess yourself a lot. Context switching is quite exhausting for me, and I prefer to immerse myself in one project and get it to the magical point where it is "done." However, when there are five or six balls you have to juggle at once, you're always afraid of blocking someone else, or leaving a customer unhappy.

On the flip side, this is a strong indication of trust that the leaders trust the individual contributors enough to give them this degree of responsibility. It's a learning experience, but I feel incredibly lucky to be working where I am now! I recently had a conversation with our CTO that really stuck with me. He said three things, one after the other:

> This is probably one of the most highly efficient engineering teams I've worked with in my career.

> I think we're operating at... maybe 50% of our possible efficiency.

> We can get higher without working longer hours and just working better as a team, I think.

Three bombshells, right after the other. I was struck at how grand and compelling this vision was, and how deep his trust in the team went.

One sub-topic I find interesting is the prioritization of customer requests over foundational work. Customer requests are additive and short term: each completed request improves the relationship with a single customer (and each dropped ball hurts it). On the other hand, foundational work is multiplicative: a strong foundation improves the product for everyone; however, if individual customer requests are dropped while working on foundation, the multiplicand decreases despite the multiplier increasing, possibly resulting in a net loss of value. The gold standard is to find a way to architect customer requests such that they _become_ foundational work, and improve the platform for everyone in a way that ideally cuts down requests in this domain from other customers.

#### Vim as a code review tool

I'm playing with [using my editor as my primary code review tool](https://github.com/timhwang21/dotfiles/blob/master/settings/.gitconfig#L34).

The primary advantage is context. By viewing the diff in your editor, you can easily jump to any definition or type whose meaning isn't immediately clear from the diff. I find that when reviewing code in the browser, I often have to follow along in my editor anyways for full understanding.

Secondly, there's speed. I've used quite a few code review tools in the past, and Gitlab and Bitbucket stand out to me as handling large diffs extraordinarily poorly. In your editor, you can easily hop between files and tabs instantaneously. Directory-wide searching is trivial as well.

Finally, testing ideas is incredibly easy. I often want to double-check the accuracy of some comment I'm leaving, so I have to open my editor and navigate to the file I'm currently viewing on the browser, make my change, and test it. When your editor is your code review tool, you can make your changes the instant they come to mind, with very little friction.

The biggest source of friction I've been running into is leaving comments. Right now I've just been leaving them inline in code, and then doing a diff when complete and copying the comments into the browser. As you can imagine, this is quite annoying.

A more minor grievance is discovery. While Github has a rich command line ecosystem, it's not immediately obvious how I would query and navigate a list of all open PRs on Bitbucket, which is our current code review tool. Currently, I still have to use the browser to actually get to the PR I am reviewing to check it out.

### 2020-3-24

#### [Margin considered harmful](https://mxstbr.com/thoughts/margin)

I strongly agree with this, and am glad that I'm getting some validation about pushing to enforce this on teams I've been on previously.

I've been using CSS-grid even for single-column, two-dimensional layouts, because `grid-gap` is such a great abstraction and results in incredibly standardized layouts. And it's incredibly easy to add responsiveness if needed -- just tweak the column count based on the page size!

This is a great motivator for me to step up my technical communications skills, though. This article is succinct, yet persuasive and informative..

### 2020-3-21

#### Practical explanation of folds

People joke about the monad curse, in that once you understand monads, you lose the ability to explain them in a sensible way. Sometimes this "curse" is genericized to cover any nontrivial functional programming concept.

[lexi-lambda](https://lexi-lambda.github.io/) seems to be an exception to this rule. Here's a great explanation nested in a code review about when to use each type of fold, with concrete examples: [`foldl` vs. `foldr` illustrated](https://github.com/hasura/graphql-engine/pull/2933#discussion_r328821960)

### 2020-3-11

#### Micro brands revisited

[Why all the Warby Parker clones are now imploding](https://marker.medium.com/why-all-the-warby-parker-clones-are-now-imploding-44bfcc70a00c)

1. As more brands arise, customer acquisition gets more expensive due to increased advertising costs. Very interesting analogy of "[CAC is the new rent](https://www.inc.com/magazine/201805/tom-foster/direct-consumer-brands-middleman-warby-parker.html)".
2. Digital advertising is less mature (or maybe scaleable) than anyone thought? As these companies scale up, they start finding that physical billboards in high-traffic locations are more effective than digital advertising. Similarly, brands that sought to bypass having to be physically stocked are now fighting for those very spots.
3. Associated with the above, overly aggressive emphasis on growth due to heavy courting of VCs.

This conclusion somewhat parallels findings from the gig economy:

1. Tech investments do allow you to cut costs and be more nimble... but seemingly only up to a certain scale.
2. Part of the reason costs are lower in the first place are solely due to undersaturation of the digital space.

Another interesting case study of Casper as a rare public micro brand follows. I won't summarize it here, but worth a read.

### 2020-3-8

#### Vim dispatch

(Seedlings of a future reference page.)

Useful dispatch recipes:

```sh
:Dispatch yarn
:Dispatch yarn jest %
:Dispatch rubocop -a %
:Dispatch docker-compose restart
:Dispatch git fetch
```

Caveats:

- Does not work for tty-needing commands, e.g. tig, `git commit`...
- Does not work with aliases
- Note that vim-fugitive already handles pull, push, etc. asynchronously.

## January

### 2020-1-20

#### Haskell's regexp library

...is probably the most expressive I've ever seen.

```haskell
module Main where

import           Text.Regex.Posix

match = "fee fi fo fum" =~ "fo"
-- ()
-- Returns empty? Or errors without FlexibleContexts. Type inference fails. Or does it?

-- By specifying boolean, we get T/F depending on if a match exists
match :: Bool
-- True

-- Or we can specify Int and get the match count
match :: Int
-- 1

-- Or we can specify String and get the match text
match :: String
-- "fo"

-- Ask for a (Int,Int) tuple and we get the match index and length
match :: (Int, Int)
-- (7,2)
```

There are several interesting things to unpack here. Right off the bat, we have a function with a polymorphic return value in a strict statically typed language. That alone is notable. How is this possible?

Let's take a closer look. When inspecting the type of `=~` with `:t (=~)` in GHCi, we get:

```haskell
(=~)
  :: (RegexMaker Regex CompOption ExecOption source,
      RegexContext Regex source1 target) =>
     source1 -> source -> target
```

It turns out the return value is a value of types `RegexContext` and `Regex`. Without diving deeper into the implementation of these types, we know that either `RegexContext` or `Regex` is the source of this magic polymorphism. Specifically, this is implemented using [pattern matching](http://hackage.haskell.org/package/regex-base-0.94.0.0/docs/src/Text.Regex.Base.Context.html#line-332): several instances are declared, with different return values based on the type of `target`, which is the [last type parameter of `RegexContext`](http://hackage.haskell.org/package/regex-base-0.94.0.0/docs/src/Text.Regex.Base.RegexLike.html#RegexContext). In regular usage, if the expected type of `target` is clear from context, no type annotation is needed: the type system just "does the right thing."

In an object oriented language, `RegexContetxt` might be implemented as a class with instance methods like `.asBool()`, `.asTuple()`, etc. However, the consumer is still responsible for calling the correct method, whereas here the return type logic lives entirely at the type level! There is no runtime cost at all. This results in a beautifully elegant developer experience.

### 2020-1-16

#### A somewhat elegant way to ensure list nonemptiness

When working with lists with an indeterminate length, it often makes sense to treat the list as being of type `[Maybe t]`.

Thus, the following idiom can be used to concisely ensure nonemptiness without using `Data.List.NonEmpty` or a related type.

```haskell
list1 :: [Maybe Int]
list1 = [Just 1, Just 2, Just 3]
maximum list1
-- Just 3

list2 :: [Maybe Int]
list2 = []
maximum list2
-- Error!

safeMaximum = maximum . (Nothing:) -- cons-es Nothing to any list
safeMaximum list1
-- Just 3
safeMaximum list2
-- Nothing
```

### 2020-1-15

#### [Blue-green deploys with K8s](https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/)

![Diagram](https://d33wubrfki0l68.cloudfront.net/5a87649bfab8bd84f95c288e8eb0f01c52274e12/7dd66/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/resources.png)

### 2020-1-4

#### [OSX: Have Finder open files in vim](https://thepugautomatic.com/2015/02/open-in-iterm-vim-from-finder/)

Slightly modified and added to my dotfiles [here](https://github.com/timhwang21/dotfiles/commit/e221fd7df5d78b38acd7fd0aa12d4ed5e8713300), though it's not really a dotfile. TODO: Find a better solution for including self-written scripts and applications for new machine setup.

Source: (Applescript really is the ugliest language ever.)

```applescript
on run {input, parameters}
	set filename to POSIX path of input
	set cmd to "clear;cd `dirname " & filename & "`;vim " & filename
	tell application "iTerm"
		tell the current window
			create tab with default profile
			tell the current session
				write text cmd
			end tell
		end tell
	end tell
end run
```

### 2020-1-3

#### Bolster tests while refactoring

I ran into an idea that's novel to me today, that is apparently common in the Haskell community. When refactoring, the old implementation is kept around in its entirety, and then acts as a test model for the new implementation for as long as is necessary. This is _slightly_ more robust than just keeping around some list of fixtures for specs.

This is partially enabled by the Haskell test ecosystem having good DX around autogenerating primitive fixtures and running a barrage of them against your code, but that's a tractable problem in other languages.

### 2020-1-2

#### Stripe's ecosystem

As a user, Stripe's accounts ecosystem is a thing of beauty. Pay for something with Stripe once, and if you visit the checkout page of another vendor using Stripe, you are automatically contacted with a verification code that imports your payment details automatically. Shopify's system is somewhat similar but Stripe's implementation is so low friction it actually brought a tear to my eye. Will definitely be drawing inspiration from here for work.

### 2020-1-1

#### [Chain of Command](https://github.com/timhwang21/chain-of-command)

After months of procrastination, I've finally gotten this to a feature-complete v1 stage. It's a fairly simple Haskell program that analyzes your shell history to help you identify areas you can optimize with aliases.
